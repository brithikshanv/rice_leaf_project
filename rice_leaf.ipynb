{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd57e4b",
   "metadata": {},
   "source": [
    "## **RICE LEAF DISEASE DETECTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f69343c",
   "metadata": {},
   "source": [
    "Rice is a major food crop, and diseases affecting rice leaves can significantly\n",
    "reduce crop yield. Early and accurate detection of rice leaf diseases helps farmers\n",
    "take timely action.\n",
    "\n",
    "This project aims to classify rice leaf images into three disease categories:\n",
    "- Leaf Smut\n",
    "- Brown Spot\n",
    "- Bacterial Leaf Blight\n",
    "\n",
    "With the use of image processing and deep learning techniques to build and evaluate\n",
    "classification models.\n",
    "\n",
    "### Dataset :\n",
    "\n",
    "- Total Images: 120\n",
    "- Classes: 3\n",
    "- Images per class: 40\n",
    "- Image format: JPG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a3e17",
   "metadata": {},
   "source": [
    "### **IMPORT LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03586873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/brithiksha/Documents/PROJECTS/rice_leaf_project/venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76316289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS and file handling\n",
    "import os\n",
    "\n",
    "# Numerical computations and plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "\n",
    "# TensorFlow / Keras for deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNetV2, ResNet50, EfficientNetB0\n",
    "\n",
    "# Sklearn metrics for evaluation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a1cdf",
   "metadata": {},
   "source": [
    "### **DATASET PATH AND CLASS IDENTIFICATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "829117b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disease classes found: ['.DS_Store', 'Bacterial leaf blight', 'Leaf smut', 'Brown spot']\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: 'rice_leaf_dataset/.DS_Store'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m classes:\n\u001b[1;32m     11\u001b[0m     class_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     images \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(valid_ext)]\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of images in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'rice_leaf_dataset/.DS_Store'"
     ]
    }
   ],
   "source": [
    "# Path to dataset\n",
    "dataset_path = \"rice_leaf_dataset/\"\n",
    "\n",
    "# List all classes (folder names)\n",
    "classes = os.listdir(dataset_path)\n",
    "print(\"Disease classes found:\", classes)\n",
    "\n",
    "# Count the number of images in each class\n",
    "valid_ext = ('.jpg', '.jpeg', '.png')\n",
    "for cls in classes:\n",
    "    class_path = os.path.join(dataset_path, cls)\n",
    "    images = [f for f in os.listdir(class_path) if f.lower().endswith(valid_ext)]\n",
    "    print(f\"Number of images in '{cls}': {len(images)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0608eaea",
   "metadata": {},
   "source": [
    "### **IMAGE VISUALIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4f7050",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: 'rice_leaf_dataset/.DS_Store'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes):\n\u001b[1;32m      4\u001b[0m     class_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     image_name \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_path\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Take first image\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(class_path, image_name))\n\u001b[1;32m      8\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'rice_leaf_dataset/.DS_Store'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display one sample image from each disease class\n",
    "plt.figure(figsize=(12,4))\n",
    "for idx, cls in enumerate(classes):\n",
    "    class_path = os.path.join(dataset_path, cls)\n",
    "    image_name = os.listdir(class_path)[0]  # Take first image\n",
    "    img = Image.open(os.path.join(class_path, image_name))\n",
    "    \n",
    "    plt.subplot(1,3,idx+1)\n",
    "    plt.imshow(img)  # Show image\n",
    "    plt.title(cls)   # Display class name\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb372aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count images per class\n",
    "image_counts = {}\n",
    "valid_ext = ('.jpg', '.jpeg', '.png')\n",
    "\n",
    "for cls in classes:\n",
    "    class_path = os.path.join(dataset_path, cls)\n",
    "    images = [f for f in os.listdir(class_path) if f.lower().endswith(valid_ext)]\n",
    "    image_counts[cls] = len(images)\n",
    "\n",
    "# Convert to pandas Series for plotting\n",
    "image_counts_series = pd.Series(image_counts)\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(6,4))\n",
    "image_counts_series.plot(kind='bar', color='skyblue')\n",
    "plt.title(\"Number of Images per Class\")\n",
    "plt.xlabel(\"Disease Class\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# Histogram shows if any class is underrepresented, affects model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7b863a",
   "metadata": {},
   "source": [
    "#### **VERIFY IMAGE RESOLUTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79291c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store image resolutions\n",
    "resolutions = []\n",
    "\n",
    "for cls in classes:\n",
    "    class_path = os.path.join(dataset_path, cls)\n",
    "    for img_file in os.listdir(class_path):\n",
    "        if img_file.lower().endswith(valid_ext):\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "            img = Image.open(img_path)\n",
    "            resolutions.append(img.size)  # (width, height)\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "res_df = pd.DataFrame(resolutions, columns=['Width', 'Height'])\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"Image Resolution Summary:\")\n",
    "print(res_df.describe())\n",
    "\n",
    "# Optional: plot histogram of widths and heights\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(res_df['Width'], bins=10, color='skyblue')\n",
    "plt.title(\"Image Width Distribution\")\n",
    "plt.xlabel(\"Width (pixels)\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(res_df['Height'], bins=10, color='lightgreen')\n",
    "plt.title(\"Image Height Distribution\")\n",
    "plt.xlabel(\"Height (pixels)\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Resolution check ensures CNN input preprocessing is appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5740b9be",
   "metadata": {},
   "source": [
    "#### Observations on Dataset Distribution\n",
    "\n",
    "- **Bacterial leaf blight** has **40 images**, indicating a well-represented class.\n",
    "- **Brown spot** also has **40 images**, matching Bacterial leaf blight in sample size.\n",
    "- **Leaf smut** has **39 images**, which is slightly fewer than the other two classes.\n",
    "\n",
    "##### Overall Insight\n",
    "- The dataset is **nearly balanced** across the three disease categories.\n",
    "- The maximum difference between classes is **only 1 image**, which is negligible.\n",
    "- This balanced distribution is favorable for **training machine learning or deep learning models**, as it helps reduce class bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0f95c5",
   "metadata": {},
   "source": [
    "### **IMAGE SIZE ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5c07b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store image dimensions\n",
    "image_sizes = []\n",
    "\n",
    "# Collect size of every image in the dataset\n",
    "for cls in classes:\n",
    "    class_path = os.path.join(dataset_path, cls)\n",
    "    \n",
    "    for img_name in os.listdir(class_path):\n",
    "        img_path = os.path.join(class_path, img_name)\n",
    "        img = Image.open(img_path)\n",
    "        image_sizes.append(img.size)  # (width, height)\n",
    "\n",
    "# Find unique image sizes\n",
    "unique_sizes = np.unique(image_sizes, axis=0)\n",
    "\n",
    "print(\"Unique image sizes in the dataset:\")\n",
    "print(unique_sizes)\n",
    "\n",
    "\n",
    "# Confirm image have different resoulutions.\n",
    "# Justifies need for resizing before model training.\n",
    "# Required explicitly for CNN input constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b8f176",
   "metadata": {},
   "source": [
    "### **IMAGE PREPROCESSING AND DATA AUGMENTATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a64a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image size and batch size for training\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Data generator with augmentation for training\n",
    "data_generator = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,          # Normalize pixel values\n",
    "    rotation_range=25,          # Rotate images\n",
    "    width_shift_range=0.2,      # Horizontal shift\n",
    "    height_shift_range=0.2,     # Vertical shift\n",
    "    zoom_range=0.2,             # Zoom in/out\n",
    "    horizontal_flip=True,       # Flip images\n",
    "    validation_split=0.2        # 80% train, 20% validation\n",
    ")\n",
    "\n",
    "# WHY AUGMENTATION?\n",
    "# Dataset is very small (119 images).\n",
    "# Augmentation artificially increases dataset size.\n",
    "# Prevents overfitting during model training.\n",
    "# Improves model generalization to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefbe4f7",
   "metadata": {},
   "source": [
    "### **TRAIN AND VALIDATE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c27662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data generator\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical', # Multi-class classification\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Validation data generator\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Automatically assigns labels based on folder names.\n",
    "# Efficient memory usage.\n",
    "# Applies real-time data augmentation to training images.\n",
    "# Prevent overfitting and improve model robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68530e1",
   "metadata": {},
   "source": [
    "### **MODEL TRAINING FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e45f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train_model(base_model, model_name, epochs=15):\n",
    "    \"\"\"\n",
    "    Build, train, and evaluate a transfer learning model\n",
    "    \"\"\"\n",
    "    # Freeze pre-trained base layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Add custom classification layers on top of the base\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)  # Global pooling reduces features\n",
    "    x = Dropout(0.3)(x)               # Regularization\n",
    "    x = Dense(128, activation='relu')(x)  # Dense layer for learning patterns\n",
    "    output = Dense(3, activation='softmax')(x)  # Output for 3 classes\n",
    "    \n",
    "    # Create the final model\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining {model_name}...\\n\")\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=validation_generator,\n",
    "        epochs=epochs,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Extract final validation accuracy\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    \n",
    "    return model, val_acc, history\n",
    "\n",
    "# Reusable function for all transfer learning models, keeps notebook clean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e212658",
   "metadata": {},
   "source": [
    "### **TRAIN MULTIPLE MODELS**\n",
    "\n",
    "Compare multiple models to choose best tradeoff between accuracy & efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b0cbeb",
   "metadata": {},
   "source": [
    "#### **MOBILENETV2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b943a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_base = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "mobilenet_model, mobilenet_acc, mobilenet_history = build_and_train_model(mobilenet_base, \"MobileNetV2\")\n",
    "print(\"MobileNetV2 Validation Accuracy:\", mobilenet_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fff937",
   "metadata": {},
   "source": [
    "#### **RESNETE50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7978e71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "resnet_model, resnet_acc, resnet_history = build_and_train_model(resnet_base, \"ResNet50\")\n",
    "print(\"ResNet50 Validation Accuracy:\", resnet_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a4c530",
   "metadata": {},
   "source": [
    "#### **EFFICIENTNETB0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd74ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_base = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "efficientnet_model, efficientnet_acc, efficientnet_history = build_and_train_model(efficientnet_base, \"EfficientNetB0\")\n",
    "print(\"EfficientNetB0 Validation Accuracy:\", efficientnet_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fae0c4",
   "metadata": {},
   "source": [
    "### **MODEL COMPARISON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34866be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare validation accuracy across models\n",
    "model_comparison = pd.DataFrame({\n",
    "    \"Model\": [\"MobileNetV2\", \"ResNet50\", \"EfficientNetB0\"],\n",
    "    \"Validation Accuracy\": [mobilenet_acc, resnet_acc, efficientnet_acc]\n",
    "})\n",
    "model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4928fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart for easy visualization\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(model_comparison[\"Model\"], model_comparison[\"Validation Accuracy\"])\n",
    "plt.title(\"Validation Accuracy Comparison\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0,1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc15574",
   "metadata": {},
   "source": [
    "Shows why MobileNetV2 is chosen (lightweight + comparable accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe9994c",
   "metadata": {},
   "source": [
    "### **MODEL EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9132e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset validation generator\n",
    "validation_generator.reset()\n",
    "\n",
    "# Get predicted probabilities\n",
    "preds = mobilenet_model.predict(validation_generator)\n",
    "\n",
    "# Convert to class indices\n",
    "pred_classes = np.argmax(preds, axis=1)\n",
    "\n",
    "# True labels\n",
    "true_classes = validation_generator.classes\n",
    "\n",
    "# Class names\n",
    "class_labels = list(validation_generator.class_indices.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e5dce",
   "metadata": {},
   "source": [
    "#### **CONFUSION MATRIX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5693ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_classes, pred_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix â€“ MobileNetV2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18284002",
   "metadata": {},
   "source": [
    "#### **CLASSIFICATION REPORT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb4549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get precision, recall, F1-score per class\n",
    "report = classification_report(true_classes, pred_classes, target_names=class_labels)\n",
    "print(\"Classification Report:\\n\")\n",
    "print(report)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "report_dict = classification_report(true_classes, pred_classes, target_names=class_labels, output_dict=True)\n",
    "performance_df = pd.DataFrame(report_dict).transpose()\n",
    "performance_df[['precision','recall','f1-score']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796fd5aa",
   "metadata": {},
   "source": [
    "Shows per-class performance and proves robustness of chosen model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a737399",
   "metadata": {},
   "source": [
    "### **OVERFITTING ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training vs validation accuracy\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(mobilenet_history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(mobilenet_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot training vs validation loss\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(mobilenet_history.history['loss'], label='Training Loss')\n",
    "plt.plot(mobilenet_history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a0c1bb",
   "metadata": {},
   "source": [
    "### **ERROR ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8850d1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of misclassified images\n",
    "mis_idx = np.where(pred_classes != true_classes)[0]\n",
    "\n",
    "# Visualize first 6 misclassified images\n",
    "plt.figure(figsize=(12,6))\n",
    "for i in range(min(6,len(mis_idx))):\n",
    "    idx = mis_idx[i]\n",
    "    img_path = validation_generator.filepaths[idx]\n",
    "    img = Image.open(img_path)\n",
    "    true_label = class_labels[true_classes[idx]]\n",
    "    pred_label = class_labels[pred_classes[idx]]\n",
    "    \n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"True: {true_label}\\nPred: {pred_label}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Helps understand model weakness and visualize mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c5dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model in Keras recommended format\n",
    "mobilenet_model.save(\"rice_leaf_mobilenetv2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57bccae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
